---
#--- official ---#
# Hausarbeit im Seminar: Dynamiken postdeutscher Gegenwartsliteratur, Clara Liso, SS2022 FUB
# im Fach: Allgemeine und Vergleichende Literaturwissenschaften
# eingereicht von Stephan Schwarz
# Abgabe: due

title: "DYN HA / SS22 FUB / Clara Liso"
author: "St. Schwarz"
date: "`r Sys.Date()`"
zotero: AVL_dyn
output: 
  bookdown::html_document2:
    global_numbering: TRUE
    number_sections: TRUE
    code_download: yes
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
    self_contained: FALSE #TRUE for standalone html page knit
    #css: ../style_HA.css
#swap / decomment ff .bib/.css references for self compile .Rmd to html
#bibliography: https://raw.githubusercontent.com/esteeschwarz/DH_essais/main/sections/DYN/DYN_HA/DYN_HA.bib
    css: https://ada-sub.rotefadenbuecher.de/skool/public/papers/011/style_HA.css
bibliography: DYN_HA.bib
---
# A. head
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#mini critical: dont find relative files in path
#knitr::opts_knit$set(root.dir="~/boxHKW/21S/DH")
#knitr::opts_knit$set(base.dir = "~/boxHKW/21S/DH/gith/DH_essais/sections/DYN/DYN_HA")
#EWA, lapsi
#knitr::opts_knit$set(root.dir="~/boxHKW/UNI/21S/DH")
#knitr::opts_knit$set(base.dir = "~/boxHKW/UNI/21S/DH/gith/DH_essais/sections/DYN/DYN_HA")

```

```{r eval=TRUE, echo=FALSE, warning=FALSE,message=FALSE}
library(httr)
#fetch zotero .bib online
share <- runif(1)
x<-GET("https://api.zotero.org/groups/4713246/collections/9LNRRJQN/items/top?format=bibtex")
bib<-content(x,"text")
y<-tempfile("ref",fileext = ".bib")
writeLines(bib,y)
```

```{r eval=TRUE, echo=FALSE, warning=FALSE,message=FALSE}
library(mongolite)
library(jsonlite)
library(syuzhet)
library(readr)
library(ggplot2)
library(lme4)
library(stringi)



#root<-"/Users/lion/boxHKW/21S/DH/"
#local<-paste0(root,"local/DYN/")
#setwd("~/boxHKW/21S/DH")
#src<-"wolf_FF_1.json"
#src<-paste0(local,"wolf_FF-LEN_1.json")
#getwd()
src<-"../../../../../local/R/cred_gener.csv"
  cred<-read.csv(src)
#  con<- mongo(collection = "wolfdb003", db ="deadend", url=cred$url[cred$q=="mongodb"])
 #   dta<-con$find('{}')
#local data:
    src<-"../../../../../local/DYN/db/wolfdb003.csv"

    dta<-read.csv(src)
    dta_sf<-dta
#k<-2
#dtatxt<-as.data.frame(fromJSON(src))
#dtatxt<-(fromJSON(src))
#cat("load TEI from:",src)
    src<-"DYN_HA_semantics.R"
#source("/gith/DH_essais/sections/DYN/DYN_HA/DYN_HA_semantics.R")
   source(src)
  lc<-length(dta$content)-1

    
```

---

# einleitung

# u. wolf

# work
## corpus aufbereitung
Das Korpus, welches nach Digitalisierung der Buchvorlagen aus einer Datenbank abgerufen wird, enthält `r length(dta_sf$content)` Einträge, nach Abzug der Kapitelüberschriften und Zitate `r length(dta$content)-1` Datensätze (Texte), die zur Auswertung herangezogen werden können.
Für die Analyse wurde das gesamte (publizierte) lyrische Werk Uljana Wolfs, bestehend aus, in der Reihenfolge des Erscheinens:

- kochanie, ich habe Brot gekauft (2005)
- falsche freunde (2009)
- meine schönste lengevitch (2013)

herangezogen.

## basic statistics
```{r echo = FALSE,warning=FALSE}
# wolfcpt<-dta$content
# wolftokens<-stri_count_boundaries(wolfcpt,"word")
# dta$tokens<-wolftokens
# tk1<-dta$tokens[dta$book_id==1]
# tk2<-dta$tokens[dta$book_id==2]
# tk3<-dta$tokens[dta$book_id==3]
# stk1<-sum(tk1,na.rm = T)
# stk2<-sum(tk2,na.rm = T)
# stk3<-sum(tk3,na.rm = T)
# stkcpt<-sum(dta$tokens,na.rm = T)
# wolfcpt_s<-stri_split_boundaries(dta$content,type="word")
# wolfcpt_s
md<-median(dta_t$tokens) #durchschnittliche textlänge
#dta_t$types
#set$types
```

Die `r lc` Texte (Lyrik und lyrische Prosa) haben einen Umfang von ``r sum(dta_t$tokens)`` Wörtern (tokens), die sich in ``r sum(dta_t$types)`` distinct types einteilen lassen, die type/token ratio beträgt demnach ``r sum(dta_t$types)/sum(dta_t$tokens)``. Die durchschnittliche Textlänge (median) beträgt ``r md`` Wörter. Wir haben noch keine Vergleichswerte, die sinnvoll wären...
Mit der entstandenen Textmatrix (cf. Table \@ref(tab:text-matrix)) läszt sich noch weiter rechnen.


```{r text-matrix, echo = F,warning=FALSE}
knitr::kable(wolfmatrix[1:10, 1:8], caption = "*simplest matrix of text beginnings*")
end<-50
```

Zum Beispiel lassen sich die Worgleichungen visualisieren, die an bestimmten Positionen des Textes aufscheinen. Die peaks in der folgenden Graphik zeigen Wortpositionen von Position 1 bis `r end`, an denen von Wolf die meisten analogen Wörter verwendet wurden. 


```{r text-sim-gr,fig.cap="similarity of text beginnings", echo = F,warning=FALSE}
#end<-50
plot(typearray[1:50],type="h",xlab = paste0("first ", end," words of texts"),ylab="count of distinct types")
```


## sentiment analysis
Für jeden einzelnen Text liesz sich ein Wert bestimmen (zur Berechnung cf. [[@jockers_2015]](https://www.matthewjockers.net/2015/02/02/syuzhet/)), der Aussagen darüber zuläszt, in welchem Spektrum (positive/negative sentiment) sich dieser verorten läszt.
Die absoluten *sentiment values* werden weiterhin durch Fourier-Transformation auf die Gesamtheit der gemessenen Werte projiziert, um eine glatte, von den absoluten Werten unabhängige Darstellung zu ermöglichen. (cf. Figure \@ref(fig:sent-freq)).
Welche Aussagen sich aus diesen Erkenntnissen ableiten lassen sollen, ist mir noch nicht ganz klar. I will elaborate on that.

### visualisation
```{r eval=TRUE, echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
#calls miracle function from external script:
#book<-3
#chapter<-3
#plot_abs<-0
#text<-2
#dtatarget<-plotsentiment(dtatxt,book,chapter,plot_abs,3) #ARG: (set,book,chapter,absolute)
le<-do_sentiment(dta)
dta_t<-get_types(dta,1)
mfw_dta<-list()
for (k in 2: length(chapter_ex)){
  

mfw_dta[[k]]<-mfw(dta,k)
}
#mfw_dta[[3]]$max$id
```

 
 
```{r sent-abs, fig.cap="absolute *sentiment values* über das gesamte Textkorpus",echo=FALSE}
 # sent2<-c(a1,a2,a3) #untransformed sentiment, absolute
  scatter.smooth(1:length(le$sentiment$cpt_s2),le$sentiment$cpt_s2,.1,.1,type="h",family = "gaussian",ylab="absolute sentiment vaules",xlab="text corpus",main="sentiment over texts")
```

```{r sent-koch, fig.cap="absolute *sentiment values* über *kochanie ich habe brot gekauft*",echo=FALSE}
  scatter.smooth(1:length(le$sentiment$a1),le$sentiment$a1,.1,.1,type="h",family = "gaussian",ylab="absolute sentiment vaules",xlab="text: kochanie",main="sentiment over texts")
```

```{r sent-ff, fig.cap="absolute *sentiment values* über *falsche freunde*",echo=FALSE}
  scatter.smooth(1:length(le$sentiment$a2),le$sentiment$a2,.1,.1,type="h",family = "gaussian",ylab="absolute sentiment vaules",xlab="text: FF",main="sentiment over texts")
```

```{r sent-leng, fig.cap="absolute *sentiment values* über *meine schönste lengevitch*",echo=FALSE}
  scatter.smooth(1:length(le$sentiment$a3),le$sentiment$a3,.1,.1,type="h",family = "gaussian",ylab="absolute sentiment vaules",xlab="text: lengevitch",main="sentiment over texts")
#  sent3<-get_transformed_values(sent2)
```

```{r sent-abs-cpt, fig.cap="absolute *sentiment values* over corpus",echo=FALSE}
  p<-ggplot(dta, aes(1:length(sentiment), sentiment, colour = book)) + 
    geom_line()
  p +  labs(x="corpus", y="sentiment absolute", title="sentiment over texts", fill="book")
```

```{r sent-freq, fig.cap="frequenzanalysierte (Fourier) *sentiment values* über Korpus",echo=FALSE}
  plot(le$sentiment$cpt_s1,type = "h",col=2,ylab = "sentiment frequencies",xlab="corpus",main="sentiment over texts")
```

```{r sent-fr-cpt, fig.cap="frequency analysis cf. [[@jockers_2015]](https://www.matthewjockers.net/2015/02/02/syuzhet/)",echo=FALSE}
  plot(le$sentiment$cpt_s3,type = "h",col=2,ylab = "sentiment frequencies",xlab="corpus agglomerated",main="sentiment analysis over texts")
```

```{r sent-lm1, fig.cap="linear regression of sentiment dependencies, absolute",echo=FALSE}
  scatter.smooth(1:length(le$lm$sum$residuals),le$lm$sum$residuals,.1,.1,type="h",family = "gaussian",ylab="dependencies: lmer sentiment residuals",xlab="text corpus",main="sentiment book/chapter dependency")
```

```{r sent-lm-fr, fig.cap="linear regression of sentiment dependencies, relative",echo=FALSE}
  plot(le$lm$scaled,type = "h",col=2,ylab = "sentiment frequencies",xlab="corpus",main="sentiment analysis over texts")
```

### data
summary:
```{r echo=F}
print (mfw_dta)
exc<-3
```
```{css echo=F}
pre {max-height: 400px;}
```
Zum Beispiel: Die höchsten *sentiment* Werte, hier in [chapter:] ``r mfw_dta[[exc]]$max$chapter``, lassen sich in [text:] ``r mfw_dta[[exc]]$max$head`` finden. Die *most frequent words* dieses Abschnitts sind [duplicates:] ``r mfw_dta[[exc]]$max$words``, 
die niedrigsten finden sich im Text ``r mfw_dta[[exc]]$min$head`` 
mit ``r mfw_dta[[exc]]$min$words``.

## varianzen

---

# B. REF:
