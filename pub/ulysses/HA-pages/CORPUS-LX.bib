
@inproceedings{schmid_estimation_2008,
	address = {Manchester, United Kingdom},
	title = {Estimation of conditional probabilities with decision trees and an application to fine-grained {POS} tagging},
	volume = {1},
	isbn = {978-1-905593-44-6},
	url = {http://portal.acm.org/citation.cfm?doid=1599081.1599179},
	doi = {10.3115/1599081.1599179},
	abstract = {We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of ﬁne-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed stateof-the-art POS taggers.},
	language = {en},
	urldate = {2023-04-08},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Computational} {Linguistics} - {COLING} '08},
	publisher = {Association for Computational Linguistics},
	author = {Schmid, Helmut and Laws, Florian},
	year = {2008},
	keywords = {HU-LX, corpus, pos-tag},
	pages = {777--784},
}

@article{zeldes_annis3_nodate,
	title = {{ANNIS3} - {Multiple} {Segmentation} {Corpora} {Guide}},
	language = {en},
	author = {Zeldes, Amir},
	keywords = {annis},
}

@book{pienemann_cross-linguistic_2005,
	address = {Amsterdam},
	series = {Studies in bilingualism},
	title = {Cross-{Linguistic} {Aspects} of {Processability} {Theory}},
	isbn = {978-90-272-4141-2},
	abstract = {Seven years ago Manfred Pienemann proposed a novel psycholinguistic theory of language development, Processability Theory (PT). This volume examines the typological plausibility of PT. Focusing on the acquisition of Arabic, Chinese and Japanese the authors demonstrate the capacity of PT to make detailed and verifiable predictions about the developmental schedule for each language. This cross-linguistic perspective is also applied to the study of L1 transfer by comparing the impact of processability and typological proximity. The typological perspective is extended by including a comparison of different types of language acquisition. The architecture of PT is expanded by the addition of a second set of principles that contributes to the formal modeling of levels of processability, namely the mapping of argument-structure onto functional structure in lexical mapping theory. This step yields the inclusion of a range of additional phenomena in the processability hierarchy thus widening the scope of PT.},
	language = {eng},
	publisher = {John Benjamins Publishing Company},
	author = {Pienemann, Manfred},
	collaborator = {Pienemann, Manfred},
	year = {2005},
	keywords = {Linguistics, Cognition, Language acquisition, Psycholinguistics, Arabic language, Argument structure, Chinese languages, Cognitive psychology, Comparative linguistics, Interlanguage (Language learning), Japanese language, Language arts, Language typology, Lexical functional grammar, Novels, Psychology, Second language acquisition, Theoretical linguistics, Transfer (Learning)},
}

@misc{noauthor_startseite_nodate,
	title = {Startseite - {Forschungszentrum} {Deutscher} {Sprachatlas}},
	url = {https://regionalsprache.de/home.aspx},
	urldate = {2023-07-09},
	file = {Startseite - Forschungszentrum Deutscher Sprachatlas:/Users/guhl/Zotero/storage/XPBH6K9L/home.html:text/html},
}

@misc{noauthor_annis_nodate,
	title = {{ANNIS} (corpus-tools.org)},
	url = {https://corpus-tools.org/annis/},
	urldate = {2023-07-22},
	file = {ANNIS (corpus-tools.org):/Users/guhl/Zotero/storage/3PFDF9EM/annis.html:text/html},
}

@article{zipser_saltnpepper_2014,
	title = {Saltnpepper {Und} {Das} {Formatpluriversum}},
	copyright = {Creative Commons Attribution 4.0, Open Access},
	url = {https://zenodo.org/record/17557},
	doi = {10.5281/ZENODO.17557},
	abstract = {These slides are about the conversion framework Pepper for linguistic data and the common meta model Salt it is based on. Further they address the problem of the multiverse of formats for linguistic data. The slides are in German.

Diese Folien beschreiben das Problem des Formatpluriversums für linguistische Daten sowie das Konverterframeork Pepper und das Metamodell Salt, die entwickelt wurden, um mit den unterscheidlichen Formaten umgehen zu können.},
	urldate = {2023-08-09},
	author = {Zipser, Florian},
	month = oct,
	year = {2014},
	note = {Publisher: Zenodo},
	keywords = {corpus, annis, formats, linguistic data, Pepper, Salt, SaltNPepper},
}

@misc{st_esteeschwarzhu-lx_2023,
	title = {esteeschwarz/{HU}-{LX}},
	url = {https://github.com/esteeschwarz/HU-LX},
	urldate = {2023-08-30},
	author = {st},
	month = jan,
	year = {2023},
	note = {original-date: 2023-01-18T07:53:57Z},
}

@misc{noauthor_part--speech_nodate,
	title = {Part-{Of}-{Speech} {Tagging} of {Modern} {Hebrew} {Textbarhaim}/{MorphTagger}...{Part}-{Of}-{Speech} {Tagging} of {Modern} {Hebrew} {Text} {Roy} {Bar}-{Haim} {Dept}. of {Computer} {Science} {Bar}-{Ilan} {University} {Ramat}-{Gan} - [{PDF} {Document}]},
	url = {https://vdocuments.site/part-of-speech-tagging-of-modern-hebrew-barhaimmorphtaggerpart-of-speech-tagging.html},
	abstract = {Natural Language Engineering 1 (1): 000–000. Printed in the United Kingdom c© 1998 Cambridge University Press 1 Part-Of-Speech Tagging of Modern Hebrew Text Roy Bar-Haim…},
	urldate = {2023-09-17},
}

@article{bar-haim_part--speech_2008,
	title = {Part-of-speech tagging of {Modern} {Hebrew} text},
	volume = {14},
	issn = {1469-8110, 1351-3249},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/partofspeech-tagging-of-modern-hebrew-text/D79EB1545B4D2B8908DC8EB3BAE997DA},
	doi = {10.1017/S135132490700455X},
	abstract = {Words in Semitic texts often consist of a concatenation of word segments, each corresponding to a part-of-speech (POS) category. Semitic words may be ambiguous with regard to their segmentation as well as to the POS tags assigned to each segment. When designing POS taggers for Semitic languages, a major architectural decision concerns the choice of the atomic input tokens (terminal symbols). If the tokenization is at the word level, the output tags must be complex, and represent both the segmentation of the word and the POS tag assigned to each word segment. If the tokenization is at the segment level, the input itself must encode the different alternative segmentations of the words, while the output consists of standard POS tags. Comparing these two alternatives is not trivial, as the choice between them may have global effects on the grammatical model. Moreover, intermediate levels of tokenization between these two extremes are conceivable, and, as we aim to show, beneficial. To the best of our knowledge, the problem of tokenization for POS tagging of Semitic languages has not been addressed before in full generality. In this paper, we study this problem for the purpose of POS tagging of Modern Hebrew texts. After extensive error analysis of the two simple tokenization models, we propose a novel, linguistically motivated, intermediate tokenization model that gives better performance for Hebrew over the two initial architectures. Our study is based on the well-known hidden Markov models (HMMs). We start out from a manually devised morphological analyzer and a very small annotated corpus, and describe how to adapt an HMM-based POS tagger for both tokenization architectures. We present an effective technique for smoothing the lexical probabilities using an untagged corpus, and a novel transformation for casting the segment-level tagger in terms of a standard, word-level HMM implementation. The results obtained using our model are on par with the best published results on Modern Standard Arabic, despite the much smaller annotated corpus available for Modern Hebrew.},
	number = {2},
	urldate = {2023-09-17},
	journal = {Natural Language Engineering},
	author = {Bar-Haim, Roy and Sima'an, Khalil and Winter, Yoad},
	month = apr,
	year = {2008},
	pages = {223--251},
}

@misc{arora_linear_2018,
	title = {Linear {Algebraic} {Structure} of {Word} {Senses}, with {Applications} to {Polysemy}},
	url = {http://arxiv.org/abs/1601.03764},
	doi = {10.48550/arXiv.1601.03764},
	abstract = {Word embeddings are ubiquitous in NLP and information retrieval, but it is unclear what they represent when the word is polysemous. Here it is shown that multiple word senses reside in linear superposition within the word embedding and simple sparse coding can recover vectors that approximately capture the senses. The success of our approach, which applies to several embedding methods, is mathematically explained using a variant of the random walk on discourses model (Arora et al., 2016). A novel aspect of our technique is that each extracted word sense is accompanied by one of about 2000 "discourse atoms" that gives a succinct description of which other words co-occur with that word sense. Discourse atoms can be of independent interest, and make the method potentially more useful. Empirical tests are used to verify and support the theory.},
	urldate = {2023-11-18},
	publisher = {arXiv},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	month = dec,
	year = {2018},
	note = {arXiv:1601.03764 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, semantics, word embedding model},
	annote = {Comment: Appear in the Transactions of the Association for Computational Linguistics 2018, link: https://transacl.org/ojs/index.php/tacl/article/view/1346},
	file = {arXiv Fulltext PDF:/Users/guhl/Zotero/storage/QSWCV8QG/Arora et al. - 2018 - Linear Algebraic Structure of Word Senses, with Applications to Polysemy.pdf:application/pdf;arXiv.org Snapshot:/Users/guhl/Zotero/storage/ZBNDSQ9U/1601.html:text/html},
}

@misc{noauthor_german_nodate,
	title = {German {Tagsets} {\textbar} {Institut} für {Maschinelle} {Sprachverarbeitung} {\textbar} {Universität} {Stuttgart}},
	url = {https://www.ims.uni-stuttgart.de/forschung/ressourcen/lexika/germantagsets/#id-cfcbf0a7-0},
	abstract = {Morphosyntactic tagsets for German},
	language = {de},
	urldate = {2023-11-19},
	keywords = {exmaralda, hulx, stts},
	file = {Snapshot:/Users/guhl/Zotero/storage/79NTDQ63/germantagsets.html:text/html},
}

@book{hirschmann_korpuslinguistik_2019,
	address = {Stuttgart},
	title = {Korpuslinguistik: {Eine} {Einführung}},
	isbn = {978-3-476-02643-9 978-3-476-05493-7},
	shorttitle = {Korpuslinguistik},
	url = {http://link.springer.com/10.1007/978-3-476-05493-7},
	language = {de},
	urldate = {2023-11-19},
	publisher = {J.B. Metzler},
	author = {Hirschmann, Hagen},
	year = {2019},
	doi = {10.1007/978-3-476-05493-7},
	keywords = {Metadaten, Annotation, Computerlinguistik, corpus linguistics, Korpora, Tagging},
	file = {Full Text PDF:/Users/guhl/Zotero/storage/UPPDG5DM/Hirschmann - 2019 - Korpuslinguistik Eine Einführung.pdf:application/pdf},
}

@incollection{noauthor_volume_2009,
	title = {Volume 2: {An} {International} {Handbook}},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	isbn = {978-3-11-021388-1},
	shorttitle = {Volume 2},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110213881.2/html},
	abstract = {In vielen Bereichen der Linguistik werden Textkorpora, Sprachkorpora oder multimodale Korpora heute als empirische Basis verwendet. Aufbauend auf Methoden des 19. Jahrhunderts haben sich dabei mit dem Aufkommen von elektronischen Korpora seit den 1940ern neue Standards f\&\#252;r linguistische Annotation und Vorverarbeitung sowie f\&\#252;r qualitative und quantitative Untersuchungen entwickelt. Das Handbuch bietet einen umfassenden \&\#220;berblick \&\#252;ber Geschichte,\&nbsp; Methoden und Anwendungen der Korpuslinguistik. Die einzelnen \&\#220;berblicks- und Spezialartikel sind von Experten und Expertinnen der jeweiligen Gebiete geschrieben. Dabei wird auf klare und umfassende Darstellung, eine gute Vernetzung zwischen den Artikel und weiterf\&\#252;hrende Hinweise Wert gelegt.},
	language = {en},
	urldate = {2023-12-02},
	booktitle = {Volume 2},
	publisher = {De Gruyter Mouton},
	month = mar,
	year = {2009},
	doi = {10.1515/9783110213881.2},
	keywords = {Computerlinguistik, Korpuslinguistik, collocations},
	file = {Full Text PDF:/Users/guhl/Zotero/storage/VW6R4Z9X/2009 - Volume 2 An International Handbook.pdf:application/pdf},
}

@article{ronan_determining_2015,
	title = {Determining {Light} {Verb} {Constructions} in {Contemporary} {British} and {Irish} {English}},
	volume = {20},
	issn = {1569-9811},
	url = {https://www.zora.uzh.ch/id/eprint/122180/1/Determining%20Light%20Verb%20Final.pdf},
	doi = {10.1075/ijcl.20.3.03ron},
	abstract = {This study implements an automated parser-based approach to the investigation of light verb constructions. The database consisting of ICE-GB and ICE-IRE is used to obtain qualitative and quantitative results on the use of light verb structures. The study explains and evaluates the steps employed to optimize parser output in detecting open lists of light verb constructions. It discusses the qualitative usage differences of these structures in the data between the two varieties and finds that ICE-GB favours fewer high frequency light verbs while ICE-IRE contains more diverse lower frequency light verbs and more passives. Overall, counts of light verb constructions are considerably higher than previously assumed. The projected counts suggest that attestations of light verb constructions will increase considerably if the search is not restricted to certain high-frequency light verbs as is typically done in studies employing manual or semi-automatic approaches to data collection.},
	language = {eng},
	number = {3},
	urldate = {2023-12-02},
	journal = {International Journal of Corpus Linguistics},
	author = {Ronan, Patricia and Schneider, Gerold},
	year = {2015},
	note = {Publisher: John Benjamins Publishing Company},
	keywords = {collocations, British English dialect, English language (Modern), Irish English dialect, light verb, parser, stylistics, syntax},
	pages = {326--354},
	file = {Akzeptierte Version:/Users/guhl/Zotero/storage/MSVZ4DS7/Ronan und Schneider - 2015 - Determining Light Verb Constructions in Contemporary British and Irish English.pdf:application/pdf},
}

@article{ronan_determining_2015-1,
	title = {Determining {Light} {Verb} {Constructions} in {Contemporary} {British} and {Irish} {English}},
	volume = {20},
	issn = {1569-9811},
	url = {https://www.zora.uzh.ch/id/eprint/122180/1/Determining%20Light%20Verb%20Final.pdf},
	doi = {10.1075/ijcl.20.3.03ron},
	abstract = {This study implements an automated parser-based approach to the investigation of light verb constructions. The database consisting of ICE-GB and ICE-IRE is used to obtain qualitative and quantitative results on the use of light verb structures. The study explains and evaluates the steps employed to optimize parser output in detecting open lists of light verb constructions. It discusses the qualitative usage differences of these structures in the data between the two varieties and finds that ICE-GB favours fewer high frequency light verbs while ICE-IRE contains more diverse lower frequency light verbs and more passives. Overall, counts of light verb constructions are considerably higher than previously assumed. The projected counts suggest that attestations of light verb constructions will increase considerably if the search is not restricted to certain high-frequency light verbs as is typically done in studies employing manual or semi-automatic approaches to data collection.},
	language = {eng},
	number = {3},
	urldate = {2023-12-02},
	journal = {International Journal of Corpus Linguistics},
	author = {Ronan, Patricia and Schneider, Gerold},
	year = {2015},
	note = {Publisher: John Benjamins Publishing Company},
	keywords = {British English dialect, English language (Modern), Irish English dialect, light verb, parser, stylistics, syntax, light verbs},
	pages = {326--354},
	file = {Akzeptierte Version:/Users/guhl/Zotero/storage/P5JZ8I6M/Ronan und Schneider - 2015 - Determining Light Verb Constructions in Contemporary British and Irish English.pdf:application/pdf},
}

@misc{noauthor_cluto_nodate,
	title = {{CLUTO} download {\textbar} {Karypis} {Lab}},
	url = {http://glaros.dtc.umn.edu/gkhome/cluto/cluto/download},
	urldate = {2023-12-03},
	keywords = {clustering},
	file = {CLUTO download | Karypis Lab:/Users/guhl/Zotero/storage/JCNMEKLM/download.html:text/html},
}

@inproceedings{klein_accurate_2003,
	address = {USA},
	series = {{ACL} '03},
	title = {Accurate unlexicalized parsing},
	url = {https://dl.acm.org/doi/10.3115/1075096.1075150},
	doi = {10.3115/1075096.1075150},
	abstract = {We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36\% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize.},
	urldate = {2023-12-02},
	booktitle = {Proceedings of the 41st {Annual} {Meeting} on {Association} for {Computational} {Linguistics} - {Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Klein, Dan and Manning, Christopher D.},
	month = jul,
	year = {2003},
	pages = {423--430},
	file = {Full Text PDF:/Users/guhl/Zotero/storage/VJ8EPBK7/Klein und Manning - 2003 - Accurate unlexicalized parsing.pdf:application/pdf},
}

@article{sariyar_recordlinkage_2010,
	title = {The {RecordLinkage} {Package}: {Detecting} {Errors} in {Data}},
	volume = {2},
	issn = {2073-4859},
	shorttitle = {The {RecordLinkage} {Package}},
	doi = {10.32614/RJ-2010-017},
	language = {eng},
	number = {2},
	journal = {The R journal},
	author = {Sariyar, Murat and Borg, Andreas},
	year = {2010},
	note = {Publisher: The R Foundation},
	keywords = {Computer science, collocates, corpuslx, stef},
	pages = {61--},
	file = {Volltext:/Users/guhl/Zotero/storage/I8QM9FIR/Sariyar und Borg - 2010 - The RecordLinkage Package Detecting Errors in Data.pdf:application/pdf},
}

@article{gilquin_what_2008,
	title = {What {You} {Think} {Ain}'t {What} {You} {Get}: {Highly} polysemous verbs in mind and language},
	shorttitle = {What {You} {Think} {Ain}'t {What} {You} {Get}},
	url = {https://dial.uclouvain.be/pr/boreal/object/boreal:75833},
	language = {en},
	urldate = {2024-01-08},
	author = {Gilquin, Gaëtanelle},
	year = {2008},
	keywords = {corpuslx, ha},
	file = {Full Text PDF:/Users/guhl/Zotero/storage/DRM9VRJS/Gilquin - 2008 - What You Think Ain't What You Get Highly polysemo.pdf:application/pdf},
}

@misc{ucsb_santa_2005,
	title = {Santa {Barbara} {Corpus} of {Spoken} {American} {English} {\textbar} {Department} of {Linguistics} - {UC} {Santa} {Barbara}},
	url = {https://www.linguistics.ucsb.edu/research/santa-barbara-corpus},
	urldate = {2024-01-08},
	journal = {SBC},
	author = {UCSB and DuBois, John W. and Wallace, L. Chafe and Meyer, Charles and Thompson, Sandra A. and Englebretson, Robert and Martey, Nii},
	year = {2005},
	keywords = {corpuslx, ha},
	file = {Santa Barbara Corpus of Spoken American English | Department of Linguistics - UC Santa Barbara:/Users/guhl/Zotero/storage/WL4WYCEI/santa-barbara-corpus.html:text/html},
}

@article{mehl_what_2021,
	title = {What we talk about when we talk about corpus frequency: {The} example of polysemous verbs with light and concrete senses},
	volume = {17},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	issn = {1613-7035},
	shorttitle = {What we talk about when we talk about corpus frequency},
	url = {https://www.degruyter.com/document/doi/10.1515/cllt-2017-0039/html},
	doi = {10.1515/cllt-2017-0039},
	abstract = {Gilquin (2008, What you think ain’t what you get: Highly polysemous verbs in mind and language. In Jean-Remi Lapaire, Guillaume Desagulier \&amp; Jean-Baptiste Guignard (eds.), From gram to mind: Grammar as cognition , 235–255. Bordeaux: Presse Universitaires de Bordeaux) reported that light uses of verbs (e.g. make use ) tend to outnumber concrete uses of the same verbs (e.g. make furniture ) in corpora, whereas concrete senses tend to outnumber light senses in responses to elicitation tests. The differences between corpus frequency and cognitive salience remain an important and much-discussed question (cf. Arppe et al. 2010, Cognitive corpus linguistics: Five points of debate on current theory and methodology. Corpora 5(1). 1–27). The question is particularly complicated because both corpus frequency and cognitive salience are difficult to define, and are often left undefined. Operationalising and defining corpus frequencies are the issues at the heart of the present paper, which includes a close, manual semantic analysis of nearly 6,000 instances of three polysemous verbs with light and concrete uses, make, take , and give , in the British component of the International Corpus of English. The paper compares semasiological frequencies like those measured by Gilquin (2008) to onomasiological frequency measurements (cf. Geeraerts 1997, Diachronic prototype semantics: A contribution to historical lexicology . Oxford: Clarendon Press). Methodologically, the paper demonstrates that these approaches address fundamentally different research questions, and offer dramatically different results. Findings indicate that corpus frequencies in speech may correlate with elicitation test results, if the corpus frequencies are measured onomasiologically rather than semasiologically; I refer to Geeraerts’s (2010, Theories of lexical semantics . Oxford: Oxford University Press) hypothesis of onomasiological salience in explaining this observation.},
	language = {en},
	number = {1},
	urldate = {2024-01-08},
	journal = {Corpus Linguistics and Linguistic Theory},
	author = {Mehl, Seth},
	month = may,
	year = {2021},
	note = {Publisher: De Gruyter Mouton},
	keywords = {semantics, corpuslx, onomasiology, prototypes},
	pages = {223--247},
	file = {Full Text PDF:/Users/guhl/Zotero/storage/55U5K4NL/Mehl - 2021 - What we talk about when we talk about corpus frequ.pdf:application/pdf},
}

@incollection{corpas_pastor_too_2020,
	address = {The Netherlands},
	title = {Too big to fail but big enough to pay for their mistakes: {A} collostructional analysis of the patterns [too {ADJ} to {V}] and [{ADJ} enough to {V}]},
	volume = {24},
	isbn = {978-90-272-0535-3},
	shorttitle = {Too big to fail but big enough to pay for their mistakes},
	language = {eng},
	booktitle = {Computational {Phraseology}},
	publisher = {John Benjamins Publishing Company},
	author = {Corpas Pastor, Gloria and Colson, Jean-Pierre},
	year = {2020},
	keywords = {corpuslx, ha, collex},
}

@misc{noauthor_computational_nodate,
	title = {Computational {Phraseology}},
	url = {https://web.p.ebscohost.com/ehost/ebookviewer/ebook/ZTAwMHh3d19fMjQ0NTM3NF9fQU41?sid=07966551-aae2-474e-aa36-7ab07f83e06f@redis&vid=0&format=EB&lpid=lp_247&rid=0},
	urldate = {2024-01-30},
	file = {ZTAwMHh3d19fMjQ0NTM3NF9fQU41?sid=07966551-aae2-474e-aa36-7ab07f83e06f@redis&vid=0&format=EB&lpid=lp_247&rid=0:/Users/guhl/Zotero/storage/8MLW2HKD/ZTAwMHh3d19fMjQ0NTM3NF9fQU41.html:text/html},
}

@misc{druskat_corpus-toolsorg_2016,
	title = {corpus-tools.org: {An} {Interoperable} {Generic} {Software} {Tool} {Set} for {Multi}-layer {Linguistic} {Corpora}. {Proceedings} of the {Tenth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC} 2016)},
	copyright = {Creative Commons Attribution 4.0, Open Access},
	url = {http://www.lrec-conf.org/proceedings/lrec2016/summaries/918.html},
	author = {Druskat, Stephan and Gast, Volker and Krause, Thomas},
	year = {2016},
	keywords = {annis, corpus, formats, linguistic data, Pepper, Salt, SaltNPepper},
}
